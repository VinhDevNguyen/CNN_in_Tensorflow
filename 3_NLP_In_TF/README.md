![](https://d2wvfoqc9gyqzf.cloudfront.net/content/uploads/2019/06/Website-TFSDesktopBanner.png)

### [Week 1: Sentiment In Text](./Week_1/)
* [x] [Slide](./Week_1/Slide/Sentiment_In_Text.pptx)
  * [x] Word based encodings
  * [x] Tokenizer
  * [x] Text to sequence
  * [x] Looking more at the Tokenizer
  * [x] Padding
* [x] [Notebook](./Week_1/Notebook/)
  * [x] [Text to sequence](./Week_1/Notebook/Text_To_Sequence.ipynb)
  * [x] [Word based encodings](./Week_1/Notebook/Word_based_encodings.ipynb)
  * [x] [Padding](./Week_1/Notebook/Padding.ipynb)
  * [x] [Working on sarcasm data](./Week_1/Notebook/Working_On_Sarcasm_Data.ipynb)
* [x] [Exercise](./Week_1/Exercise/)
  * [x] [Working on BBC News](./Week_1/Exercise/Working_On_BBC_news.ipynb)
  * [x] [Solution](./Week_1/Exercise/Solution/Working_On_BBC_news_Solution.ipynb)
### Week 2: Augmentation, a Technique to Avoid Overfitting
* [ ] Slide
  * [ ] Introduction
  * [ ] The IMDB dataset
  * [ ] Looking into the details
  * [ ] How can we use vectors?
  * [ ] More into the details
  * [ ] Remember the sarcasm dataset?
  * [ ] Building a classifier for the sarcasm dataset
  * [ ] Let's talk about the loss function
  * [ ] Pre-tokenized datasets
  * [ ] Diving into the code (part 1)
  * [ ] Diving into the code (part 2)

### Week 3: Sequence models

* [ ] Slide
  * [ ] Introduction
  * [ ] LSTMs
  * [ ] Implementing LSTMs in code
  * [ ] Accuracy and loss
  * [ ] Looking into the code
  * [ ] Using a convolutional network
  * [ ] Going back to the IMDB

### Week 4: Sequence models and literature

* [ ] Slide
  * [ ] Introduction
  * [ ] Looking into the code
  * [ ] Training the data
  * [ ] More on training the data
  * [ ] Finding what the next word should be
  * [ ] Predicting a word
  * [ ] Poetry!
  * [ ] Looking into the code